{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54d6c8f0-73c5-4f6b-8145-099841999c24",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Formatted Zone bucket already exists: formatted-zone\n",
      " Found 1 metadata files to process.\n",
      " Reading: persistent_landing/metadata/Animalia_Squamata_metadata_2025_10_29_07:14.csv\n",
      " No existing general metadata found, creating new one\n",
      " Adding 78 new rows to general metadata, total now: 78\n",
      " Formatted metadata updated successfully\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "-Arman Bazarchi-\n",
    "Formatted_Metadata\n",
    "Here we ensure the format of only text metadata we had in persistent_landing and move to formatted-zone bucket.\n",
    "we normalize the data and remove any incomplete data.\n",
    "connects to minIO, creates formatted-zone bucket, raises an error if pesistant_landing or temporal-zone does not exist.\n",
    "read data from persistent and normilize then store in formatted-zone, \n",
    "here we save a csv file, a .parquet file, a .json file, a schema summary of our text metadata for different uses in future.\n",
    "it avoids storing duplicate data in formatted-zone, also removes any temporal file in local storage.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from minio import Minio\n",
    "import pandas as pd\n",
    "import io, json, os, re, shutil\n",
    "from datetime import datetime\n",
    "\n",
    "# ==============================\n",
    "#          Functions\n",
    "# ==============================\n",
    "\n",
    "def setup_minio_client_and_buckets(minio_endpoint, access_key, secret_key, landing_zone, persist_prefix, formatted_zone):\n",
    "    # Setup MinIO client and validate/create buckets.\n",
    "    client = Minio(\n",
    "        minio_endpoint,\n",
    "        access_key=access_key,\n",
    "        secret_key=secret_key,\n",
    "        secure=False\n",
    "    )\n",
    "    \n",
    "    # Break if temporal landing or temporal-zone does not exist\n",
    "    if not client.bucket_exists(landing_zone):\n",
    "        sys.exit(\" ERROR: Root bucket 'Landing' does not exist in MinIO.\")\n",
    "    \n",
    "    persistent_objects = list(client.list_objects(landing_zone, prefix=f\"{persist_prefix}/\", recursive=False))\n",
    "    if not persistent_objects:\n",
    "        raise FileNotFoundError(f\" Required prefix '{persist_prefix}/' not found inside '{landing_zone}' bucket.\")\n",
    "    \n",
    "    # Ensure formatted-zone bucket exists\n",
    "    if not client.bucket_exists(formatted_zone):\n",
    "        client.make_bucket(formatted_zone)\n",
    "        print(f\" Created Formatted Zone bucket: {formatted_zone}\")\n",
    "    else:\n",
    "        print(f\" Formatted Zone bucket already exists: {formatted_zone}\")\n",
    "    \n",
    "    return client\n",
    "\n",
    "def load_metadata_files(client, landing_zone, persist_prefix):\n",
    "    # Load all metadata files from Persistent Landing.\n",
    "    \n",
    "    metadata_objects = [\n",
    "        obj.object_name for obj in client.list_objects(landing_zone, prefix=f\"{persist_prefix}/metadata/\", recursive=True)\n",
    "        if obj.object_name.endswith(\".csv\") or obj.object_name.endswith(\".json\")\n",
    "    ]\n",
    "    \n",
    "    if not metadata_objects:\n",
    "        raise FileNotFoundError(\" No metadata files found in Persistent Landing Zone.\")\n",
    "    \n",
    "    print(f\" Found {len(metadata_objects)} metadata files to process.\")\n",
    "\n",
    "    # search for metadata csv or json in temporal-zone\n",
    "    all_dfs = []\n",
    "    for obj_name in metadata_objects:\n",
    "        print(f\" Reading: {obj_name}\")\n",
    "        response = client.get_object(landing_zone, obj_name)\n",
    "        data = response.read()\n",
    "        response.close()\n",
    "        response.release_conn()\n",
    "        \n",
    "        try:\n",
    "            if obj_name.endswith(\".csv\"):\n",
    "                df = pd.read_csv(io.BytesIO(data))\n",
    "            elif obj_name.endswith(\".json\"):\n",
    "                json_data = json.load(io.BytesIO(data))\n",
    "                df = pd.json_normalize(json_data)\n",
    "            else:\n",
    "                continue\n",
    "            \n",
    "            all_dfs.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\" Error reading {obj_name}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    if not all_dfs:\n",
    "        raise ValueError(\" No valid metadata could be loaded.\")\n",
    "    \n",
    "    return all_dfs\n",
    "\n",
    "def normalize_and_combine_metadata(all_dfs, persist_prefix):\n",
    "    # Normalize schema and combine all metadata.\n",
    "    target_columns = [\n",
    "        \"uuid\", \"kingdom\", \"phylum\", \"class\", \"order\", \"family\",\n",
    "        \"genus\", \"species\", \"scientific_name\", \"common\",\n",
    "        \"persistent_path\",\"formatted_path\", \"image_url\"\n",
    "    ]\n",
    "    \n",
    "    for i, df in enumerate(all_dfs):\n",
    "        for col in target_columns:\n",
    "            if col not in df.columns:\n",
    "                df[col] = None\n",
    "        all_dfs[i] = df[target_columns]\n",
    "    \n",
    "    # Combine all Persistent metadata into one DataFrame\n",
    "    persistent_df = pd.concat(all_dfs, ignore_index=True)\n",
    "    \n",
    "    # Generate formatted_path same as persistent_path\n",
    "    persistent_df[\"formatted_path\"] = persistent_df[\"persistent_path\"].astype(str).str.replace(\n",
    "        f\"{persist_prefix}/images\", \"images\", regex=False\n",
    "    )\n",
    "    \n",
    "    return persistent_df\n",
    "\n",
    "def load_existing_general_metadata(client, formatted_zone, target_columns):\n",
    "    # Load existing general metadata from Formatted Zone. to skip duplicates.\n",
    "    \n",
    "    general_metadata_files = [\n",
    "        obj.object_name for obj in client.list_objects(formatted_zone, prefix=\"metadata/\", recursive=True)\n",
    "        if re.match(r\"metadata/all_metadata_.*\\.csv\", obj.object_name)\n",
    "    ]\n",
    "    \n",
    "    if general_metadata_files:\n",
    "        # Take the latest general metadata CSV\n",
    "        general_metadata_files.sort(reverse=True)\n",
    "        latest_metadata_file = general_metadata_files[0]\n",
    "        local_existing = \"temp_existing_metadata.csv\"\n",
    "        \n",
    "        # Download existing metadata\n",
    "        client.fget_object(formatted_zone, latest_metadata_file, local_existing)\n",
    "        general_df = pd.read_csv(local_existing)\n",
    "        \n",
    "        # Delete the old file from formatted-zone\n",
    "        client.remove_object(formatted_zone, latest_metadata_file)\n",
    "        # Remove old general formatted files\n",
    "        general_files_prefix = \"metadata/all_metadata\"\n",
    "        schema_files_prefix = \"metadata/schema_summary\"\n",
    "        # List all objects in formatted-zone/metadata\n",
    "        for obj in client.list_objects(formatted_zone, prefix=\"metadata/\", recursive=True):\n",
    "            if obj.object_name.startswith(general_files_prefix) or obj.object_name.startswith(schema_files_prefix):\n",
    "                client.remove_object(formatted_zone, obj.object_name)\n",
    "                print(f\" Removed old file: {obj.object_name}\")\n",
    "        \n",
    "        # Cleanup local temp\n",
    "        os.remove(local_existing)\n",
    "        print(f\" Loaded and removed existing general metadata with {len(general_df)} rows\")\n",
    "    else:\n",
    "        # No general metadata exists yet\n",
    "        general_df = pd.DataFrame(columns=target_columns)\n",
    "        print(\" No existing general metadata found, creating new one\")\n",
    "    \n",
    "    return general_df\n",
    "\n",
    "def merge_and_prepare_data(persistent_df, general_df):\n",
    "    # Merge new rows and prepare final dataset.\n",
    "    \n",
    "    new_rows = persistent_df[~persistent_df[\"uuid\"].isin(general_df[\"uuid\"])]\n",
    "    if not new_rows.empty:\n",
    "        updated_df = pd.concat([general_df, new_rows], ignore_index=True)\n",
    "        print(f\" Adding {len(new_rows)} new rows to general metadata, total now: {len(updated_df)}\")\n",
    "    else:\n",
    "        updated_df = general_df\n",
    "        print(\" No new rows to add; general metadata is up to date\")\n",
    "    \n",
    "    # Schema summary\n",
    "    schema_summary = pd.DataFrame({\n",
    "        \"column_name\": updated_df.columns,\n",
    "        \"dtype\": [str(updated_df[c].dtype) for c in updated_df.columns],\n",
    "        \"missing_values\": [updated_df[c].isna().sum() for c in updated_df.columns]\n",
    "    })\n",
    "    \n",
    "    return updated_df, schema_summary\n",
    "\n",
    "def save_unified_outputs(client, formatted_zone, updated_df, schema_summary):\n",
    "    # Save unified outputs in multiple formats.\n",
    "    timestamp = datetime.now().strftime(\"%Y_%m_%d_%H:%M\")\n",
    "    os.makedirs(\"temp_formatted\", exist_ok=True)\n",
    "    \n",
    "    local_csv = \"temp_formatted/all_metadata.csv\"\n",
    "    local_parquet = \"temp_formatted/all_metadata.parquet\"\n",
    "    local_json = \"temp_formatted/all_metadata.json\"\n",
    "    local_schema = \"temp_formatted/schema_summary.csv\"\n",
    "    \n",
    "    updated_df.to_csv(local_csv, index=False)\n",
    "    updated_df.to_parquet(local_parquet, index=False)\n",
    "    updated_df.to_json(local_json, orient=\"records\", lines=True)\n",
    "    schema_summary.to_csv(local_schema, index=False)\n",
    "    \n",
    "    # Upload to MinIO\n",
    "    client.fput_object(formatted_zone, f\"metadata/all_metadata_{timestamp}.csv\", local_csv, content_type=\"text/csv\")\n",
    "    client.fput_object(formatted_zone, f\"metadata/all_metadata_{timestamp}.parquet\", local_parquet, content_type=\"application/octet-stream\")\n",
    "    client.fput_object(formatted_zone, f\"metadata/all_metadata_{timestamp}.json\", local_json, content_type=\"application/json\")\n",
    "    client.fput_object(formatted_zone, f\"metadata/schema_summary_{timestamp}.csv\", local_schema, content_type=\"text/csv\")\n",
    "    \n",
    "    # Cleanup\n",
    "    os.remove(local_csv)\n",
    "    os.remove(local_parquet)\n",
    "    os.remove(local_json)\n",
    "    os.remove(local_schema)\n",
    "    shutil.rmtree(\"temp_formatted\")\n",
    "\n",
    "# ==============================\n",
    "#        Configuration\n",
    "# ==============================\n",
    "def process_formatted_metadata(\n",
    "    minio_endpoint = \"localhost:9000\",\n",
    "    access_key = \"admin\",\n",
    "    secret_key = \"password123\"):\n",
    "\n",
    "    landing_zone = \"temporal-zone\"\n",
    "    persist_prefix = \"persistent_landing\"\n",
    "    formatted_zone = \"formatted-zone\"\n",
    "    \n",
    "    # Setup MinIO client and buckets\n",
    "    client = setup_minio_client_and_buckets(minio_endpoint, access_key, secret_key, landing_zone, persist_prefix, formatted_zone)\n",
    "    \n",
    "    # Load all metadata files from Persistent Landing\n",
    "    all_dfs = load_metadata_files(client, landing_zone, persist_prefix)\n",
    "    \n",
    "    # Normalize schema and combine all metadata\n",
    "    persistent_df = normalize_and_combine_metadata(all_dfs, persist_prefix)\n",
    "    \n",
    "    # Check for existing general metadata in Formatted Zone\n",
    "    target_columns = [\n",
    "        \"uuid\", \"kingdom\", \"phylum\", \"class\", \"order\", \"family\",\n",
    "        \"genus\", \"species\", \"scientific_name\", \"common\",\n",
    "        \"persistent_path\",\"formatted_path\", \"image_url\"\n",
    "    ]\n",
    "    general_df = load_existing_general_metadata(client, formatted_zone, target_columns)\n",
    "    \n",
    "    # Merge new rows and prepare final dataset\n",
    "    updated_df, schema_summary = merge_and_prepare_data(persistent_df, general_df)\n",
    "    \n",
    "    # Save unified outputs\n",
    "    save_unified_outputs(client, formatted_zone, updated_df, schema_summary)\n",
    "    \n",
    "    print(\" Formatted metadata updated successfully\")\n",
    "\n",
    "process_formatted_metadata();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14dbbc16-78a4-44e2-be72-7030d6f9536d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
