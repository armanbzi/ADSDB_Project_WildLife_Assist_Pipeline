# WildLife Data Management Pipeline

A comprehensive DataOps orchestration system for wildlife data processing, featuring continuous integration, quality control, and monitoring capabilities.

## ğŸš€ Features

- **Complete Pipeline Orchestration**: Automated execution of the entire data management backbone
- **User-Friendly Interface**: Interactive menu system for easy operation
- **Quality Control**: Built-in code analysis and quality reporting
- **System Monitoring**: Real-time performance and resource monitoring
- **Error Handling**: Comprehensive error tracking and reporting
- **Flexible Execution**: Run complete workflows or individual scripts

## ğŸ“‹ Pipeline Workflow

### Recommended Workflow (DataOps Sequence):
1. **Temporal Landing** - Initial data ingestion
2. **Persistent Landing** - Data persistence layer
3. **Formatted Metadata** - Metadata standardization
4. **Formatted Images** - Image processing and standardization
5. **Trusted Metadata** - Data quality assurance
6. **Trusted Images** - Image quality validation
7. **Exploitation Metadata** - Advanced metadata processing
8. **Exploitation Images** - Advanced image processing
9. **Exploitation Multimodal** - Multimodal data integration

### Task Options:
- **Same Modality Search** - Similarity search within data types
- **Multimodal Similarity** - Cross-modal similarity analysis
- **Generative Task** - AI-powered data generation

## ğŸ› ï¸ Installation

1. **Clone the repository**:
   ```bash
   git clone <repository-url>
   cd WildLife/notebook
   ```

2. **Install dependencies**:
   ```bash
   pip install -r requirements.txt
   ```

3. **Setup MinIO** (if not already running):
   ```bash
   # Start MinIO server
   minio server /data --console-address ":9001"
   ```

## ğŸš€ Usage

### Quick Start
```bash
python orchestrate.py
```

### Configuration
The orchestrator will prompt you for:
- **MinIO Endpoint**: Your MinIO server address (e.g., `localhost:9000`)
- **Access Key**: MinIO access key
- **Secret Key**: MinIO secret key

### Menu Options

1. **Run Recommended Workflow**: Execute the complete pipeline in the optimal sequence
2. **Run Individual Script**: Execute a single script from the pipeline
3. **Run Custom Workflow**: Create and execute a custom sequence of scripts
4. **Quality Control & Code Analysis**: Analyze code quality and generate reports
5. **System Monitoring Dashboard**: View real-time system performance
6. **View Pipeline Status**: Check current pipeline execution status

## ğŸ“Š Quality Control

The system includes comprehensive quality control features:

- **Code Analysis**: Automated detection of code quality issues
- **Performance Monitoring**: Real-time system resource monitoring
- **Error Tracking**: Comprehensive error logging and reporting
- **Quality Reports**: Generated quality control reports

## ğŸ”§ Development

### Project Structure
```
WildLife/notebook/
â”œâ”€â”€ orchestrate.py              # Main orchestration script
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ setup.py                   # Package setup
â”œâ”€â”€ README.md                  # This file
â”œâ”€â”€ Temporal-Zone/             # Temporal data processing
â”‚   â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ notebooks/
â”œâ”€â”€ Formatted-Zone/            # Data formatting
â”‚   â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ notebooks/
â”œâ”€â”€ Trusted_Zone/              # Data quality assurance
â”‚   â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ notebooks/
â”œâ”€â”€ Exploitation-Zone/         # Advanced processing
â”‚   â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ notebooks/
â””â”€â”€ Multi-Modal Tasks/         # AI/ML tasks
    â”œâ”€â”€ scripts/
    â””â”€â”€ notebooks/
```

### Adding New Scripts
1. Convert your notebook to a Python script
2. Place it in the appropriate zone's `scripts/` directory
3. Update the `workflow_scripts` or `task_scripts` dictionary in `orchestrate.py`

## ğŸ“ˆ Monitoring

The system provides comprehensive monitoring:

- **System Metrics**: CPU, memory, and disk usage
- **Process Monitoring**: Individual script performance
- **Error Tracking**: Detailed error logging
- **Performance Reports**: Execution time and resource usage

## ğŸ§ª Testing

Run quality control analysis:
```bash
python orchestrate.py
# Select option 4: Quality Control & Code Analysis
```

## ğŸ“ Logs

- **Pipeline Logs**: `pipeline.log` - Complete execution log
- **Quality Report**: `quality_report.txt` - Code quality analysis
- **MinIO Config**: `minio_config.json` - Configuration storage

## ğŸ¤ Contributing

1. Follow the existing code structure
2. Add appropriate error handling
3. Include logging for monitoring
4. Update documentation as needed

## ğŸ“„ License

This project is part of the WildLife Data Management system developed for academic purposes.

## ğŸ‘¨â€ğŸ’» Author

**Arman Bazarchi** - Data Management Pipeline Orchestrator

---

*This orchestration system provides a complete DataOps environment for the WildLife Data Management Pipeline, ensuring reliable, monitored, and quality-controlled data processing operations.*
