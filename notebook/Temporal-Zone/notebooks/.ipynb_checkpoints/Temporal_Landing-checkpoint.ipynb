{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25935188-3c40-45fa-8b0e-64c6a74cd1aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter the following parameters:\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "MAX_PER_SPECIES (e.g., 30):  1\n",
      "MAX_SPECIES_PER_FAMILY (e.g., 11):  1\n",
      "Chunk interval (e.g., '5-10'):  1-2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded metadata from temporal-landing with 15 records.\n",
      " Scanning existing images in temporal-landing...\n",
      " Found 0 existing image files in temporal-landing.\n",
      " Tracking 14 families from existing metadata.\n",
      "\n",
      " Processing chunk 1%â€“2% ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "775193e4fa9049c0b3c6530bf9961585",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/213937319 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                          | 2649/2139373 [00:02<37:15, 955.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saved imperator (1/1) in family Boidae (1/1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2139373/2139373 [06:57<00:00, 5122.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Finished progressive sampling. Total images: 16\n",
      "ðŸ—‘ï¸ Removed local metadata file: metadata_final.csv\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "-Arman Bazarchi-\n",
    "Temporal_Landing\n",
    "here we get the raw data(image and metadata about each image) \n",
    "from source (imageomics/TreeOfLife-200M) and store in our temporal-zone.\n",
    "miniIO user must be \"admin\", and password \"password123\".\n",
    "as data is huge 200m, we get the data by chunks of 1% out of 100% of all data.\n",
    "so we limit the amount of getting data on each run.\n",
    "user is asked for a maximum number of species per family, limiting data to store up to a defined  \n",
    "number of species for each stored family.\n",
    "user also is asked for a maximum number of observations(images) per each stored species.\n",
    "also is asked for the chunks interval for the run, for example 5-10 will get from 5th% up to 10th% of the whole 100% of data.\n",
    "for now only retrieves images of snake families only because of not being able to store a lot of data at the moment.\n",
    "\n",
    "then code connects to minIO create a temporal-zone bucket and a subbucket 'temporal-landing', \n",
    "gets defined interval % of the train split of data,\n",
    "stores images in a folder 'images' and a csv file of the needed metadata in a folder 'metadata',\n",
    "it checks and avoids storing duplicates in temporal-zone, also deletes any temporary file and cache\n",
    "and huggingface heavy cach from local storage.\n",
    "\n",
    "\n",
    "# Login using e.g. `huggingface-cli login` to access this dataset\n",
    "\"\"\"\n",
    "\n",
    "from datasets import load_dataset\n",
    "from datasets import config\n",
    "import shutil\n",
    "from minio import Minio\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import io\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import requests\n",
    "import re\n",
    "\n",
    "# ==============================\n",
    "# 1 Configuration\n",
    "# ==============================\n",
    "def process_landing_zone(\n",
    "    MINIO_ENDPOINT = \"localhost:9000\",\n",
    "    ACCESS_KEY = \"admin\",\n",
    "    SECRET_KEY = \"password123\"):\n",
    "\n",
    "    ROOT_BUCKET = \"temporal-zone\"         \n",
    "    TEMP_PREFIX = \"temporal-landing\"     # Subbucket for temporal\n",
    "    \n",
    "    # Metadata config\n",
    "    METADATA_REMOTE_PATH = f\"{TEMP_PREFIX}/metadata/metadata_final.csv\"\n",
    "    METADATA_LOCAL = \"metadata_final.csv\"\n",
    "    \n",
    "    # Get user input for parameters\n",
    "    print(\"Please enter the following parameters:\")\n",
    "    MAX_PER_SPECIES = int(input(\"MAX_PER_SPECIES (e.g., 30): \"))\n",
    "    MAX_SPECIES_PER_FAMILY = int(input(\"MAX_SPECIES_PER_FAMILY (e.g., 11): \"))\n",
    "    chunk_interval = input(\"Chunk interval (e.g., '5-10'): \")\n",
    "    \n",
    "    # Parse chunk interval\n",
    "    start_chunk, end_chunk = map(int, chunk_interval.split('-'))\n",
    "    \n",
    "    # Global Limit\n",
    "    MAX_TOTAL_IMAGES = 100000\n",
    "    \n",
    "    # Known snake families\n",
    "    SNAKE_FAMILIES = {\n",
    "        \"Viperidae\", \"Elapidae\", \"Colubridae\", \"Pythonidae\", \"Boidae\",\n",
    "        \"Typhlopidae\", \"Lamprophiidae\", \"Natricidae\", \"Dipsadidae\",\n",
    "        \"Leptotyphlopidae\", \"Xenopeltidae\", \"Anomalepididae\", \"Loxocemidae\",\n",
    "        \"Uropeltidae\", \"Cylindrophiidae\", \"Aniliidae\", \"Acrochhordidae\",\n",
    "        \"Anomochilidae\", \"Atractaspididae\", \"Bolyeridae\"\n",
    "    }\n",
    "    \n",
    "    \n",
    "    #  Initialize MinIO client\n",
    "    client = Minio(\n",
    "        MINIO_ENDPOINT,\n",
    "        access_key=ACCESS_KEY,\n",
    "        secret_key=SECRET_KEY,\n",
    "        secure=False\n",
    "    )\n",
    "    \n",
    "    \n",
    "    #  Ensure temporal-landing and temporal-zone exist\n",
    "    if not client.bucket_exists(ROOT_BUCKET):\n",
    "        client.make_bucket(ROOT_BUCKET)\n",
    "        print(f\" Created top-level bucket: {ROOT_BUCKET}\")\n",
    "    \n",
    "    # Create temporal-landing subfolder if missing\n",
    "    temporal_exists = any(\n",
    "        obj.object_name.startswith(f\"{TEMP_PREFIX}/\")\n",
    "        for obj in client.list_objects(ROOT_BUCKET, recursive=False)\n",
    "    )\n",
    "    if not temporal_exists:\n",
    "        client.put_object(\n",
    "            ROOT_BUCKET,\n",
    "            f\"{TEMP_PREFIX}/.init\",\n",
    "            data=io.BytesIO(b\"init\"),\n",
    "            length=4,\n",
    "            content_type=\"text/plain\"\n",
    "        )\n",
    "        print(f\"âœ… Created subfolder: {TEMP_PREFIX}/ inside {ROOT_BUCKET}\")\n",
    "    \n",
    "    # ==============================\n",
    "    # 2 Check for existing metadata\n",
    "    # ==============================\n",
    "    try:\n",
    "        client.fget_object(ROOT_BUCKET, METADATA_REMOTE_PATH, METADATA_LOCAL)\n",
    "        metadata_df = pd.read_csv(METADATA_LOCAL)\n",
    "        print(f\" Loaded metadata from {TEMP_PREFIX} with {len(metadata_df)} records.\")\n",
    "    \n",
    "    except Exception:\n",
    "        metadata_df = pd.DataFrame(columns=[\n",
    "            \"uuid\",\"temporal_path\", \"kingdom\", \"phylum\", \"class\", \"order\", \"family\",\n",
    "            \"genus\", \"species\", \"scientific_name\", \"common\", \"image_url\"\n",
    "        ])\n",
    "        print(\" No metadata found in temporal-landing â€” starting fresh.\")\n",
    "    # Maintain fast lookup sets\n",
    "    existing_uuids = set(metadata_df[\"uuid\"].dropna().unique()) if \"uuid\" in metadata_df.columns else set()\n",
    "    \n",
    "    # ==============================\n",
    "    # 3 Scan existing images\n",
    "    # ==============================\n",
    "    print(\" Scanning existing images in temporal-landing...\")\n",
    "    \n",
    "    existing_image_ids = set()\n",
    "    for obj in client.list_objects(ROOT_BUCKET, prefix=f\"{TEMP_PREFIX}/images/\", recursive=True):\n",
    "        # Extract UUID from filename: temporal-landing/images/<uuid>.jpg\n",
    "        match = re.match(rf\"{TEMP_PREFIX}/images/([a-f0-9\\-]+)\\.jpg\", obj.object_name, re.IGNORECASE)\n",
    "        if match:\n",
    "            existing_image_ids.add(match.group(1))\n",
    "    \n",
    "    print(f\" Found {len(existing_image_ids)} existing image files in temporal-landing.\")\n",
    "    \n",
    "    family_species = {}\n",
    "    species_counts = {}\n",
    "    \n",
    "    # ==============================\n",
    "    # 4 Build family -> species map from metadata\n",
    "    # ==============================\n",
    "    if not metadata_df.empty:\n",
    "        for _, row in metadata_df.iterrows():\n",
    "            fam = row.get(\"family\")\n",
    "            sp = row.get(\"species\")\n",
    "            if pd.notna(fam) and pd.notna(sp):\n",
    "                family_species.setdefault(fam, set()).add(sp)\n",
    "                species_counts[sp] = species_counts.get(sp, 0) + 1\n",
    "    \n",
    "    print(f\" Tracking {len(family_species)} families from existing metadata.\")\n",
    "    \n",
    "    # ==============================\n",
    "    # 5 Progressive Chunk Loop\n",
    "    # ==============================\n",
    "    for chunk_idx in range(start_chunk, end_chunk):\n",
    "        print(f\"\\n Processing chunk {chunk_idx}%â€“{chunk_idx+1}% ...\")\n",
    "    \n",
    "        try:\n",
    "            dataset = load_dataset(\n",
    "                \"imageomics/TreeOfLife-200M\",\n",
    "                split=f\"train[{chunk_idx}%:{chunk_idx+1}%]\")  # <-- avoids writing huge Arrow files\n",
    "        except Exception as e:\n",
    "            print(f\" Skipping chunk {chunk_idx}: {e}\")\n",
    "            continue\n",
    "    \n",
    "        # getting needed metadata\n",
    "        for sample in tqdm(dataset):\n",
    "            species = sample.get(\"species\") or \"unknown\"\n",
    "            kingdom = sample.get(\"kingdom\")\n",
    "            cls = sample.get(\"class\")\n",
    "            order = sample.get(\"order\")\n",
    "            family = sample.get(\"family\") or \"unknown\"\n",
    "            img_id = sample.get(\"uuid\")\n",
    "            image_url = sample.get(\"source_url\")\n",
    "    \n",
    "            # Only save snake images for now\n",
    "            if not (\n",
    "                kingdom and kingdom.strip().lower() == \"animalia\"\n",
    "                and cls and cls.strip().lower() == \"squamata\"\n",
    "                and family in SNAKE_FAMILIES\n",
    "            ):\n",
    "                continue\n",
    "    \n",
    "            # skip if already have enough per this specie\n",
    "            count = species_counts.get(species, 0)\n",
    "            if count >= MAX_PER_SPECIES:\n",
    "                continue\n",
    "    \n",
    "            # skip if already have enough specie for this family\n",
    "            current_species_in_family = family_species.get(family, set())\n",
    "            if len(current_species_in_family) >= MAX_SPECIES_PER_FAMILY and species not in current_species_in_family:\n",
    "                continue\n",
    "    \n",
    "            try:\n",
    "                if img_id not in existing_image_ids:\n",
    "                    response = requests.get(image_url, timeout=10)\n",
    "                    response.raise_for_status()\n",
    "                    image_bytes = response.content\n",
    "    \n",
    "                    img = Image.open(io.BytesIO(image_bytes))\n",
    "                    img.verify()\n",
    "                    buffer = io.BytesIO(image_bytes)\n",
    "    \n",
    "                    # Save to MinIO \n",
    "                    object_name = f\"{TEMP_PREFIX}/images/{img_id}.jpg\"\n",
    "                    client.put_object(\n",
    "                        ROOT_BUCKET,\n",
    "                        object_name,\n",
    "                        data=buffer,\n",
    "                        length=len(image_bytes),\n",
    "                        content_type=\"image/jpeg\"\n",
    "                    )\n",
    "                    existing_image_ids.add(img_id)\n",
    "    \n",
    "                # skip duplicates\n",
    "                if img_id not in existing_uuids:\n",
    "                    # Save needed metadata\n",
    "                    record = {\n",
    "                        \"uuid\": img_id,\n",
    "                        \"temporal_path\": object_name,\n",
    "                        \"kingdom\": kingdom,\n",
    "                        \"phylum\": sample.get(\"phylum\"),\n",
    "                        \"class\": cls,\n",
    "                        \"order\": order,\n",
    "                        \"family\": family,\n",
    "                        \"genus\": sample.get(\"genus\"),\n",
    "                        \"species\": species,\n",
    "                        \"scientific_name\": sample.get(\"scientific_name\"),\n",
    "                        \"common\": sample.get(\"common\"),\n",
    "                        \"image_url\": image_url\n",
    "                    }\n",
    "    \n",
    "                    metadata_df = pd.concat([metadata_df, pd.DataFrame([record])], ignore_index=True)\n",
    "                    metadata_df.to_csv(METADATA_LOCAL, index=False)\n",
    "                    client.fput_object(ROOT_BUCKET, METADATA_REMOTE_PATH, METADATA_LOCAL, content_type=\"text/csv\")\n",
    "    \n",
    "                    species_counts[species] = count + 1\n",
    "                    family_species.setdefault(family, set()).add(species)\n",
    "                    existing_uuids.add(img_id)\n",
    "    \n",
    "                    print(f\" Saved {species} ({species_counts[species]}/{MAX_PER_SPECIES}) \"\n",
    "                          f\"in family {family} ({len(family_species.get(family, []))}/{MAX_SPECIES_PER_FAMILY})\")\n",
    "    \n",
    "            except Exception as e:\n",
    "                print(f\" Error saving {species}: {e}\")\n",
    "                continue\n",
    "    \n",
    "            if len(metadata_df) >= MAX_TOTAL_IMAGES:\n",
    "                print(\" Reached global image limit.\")\n",
    "                break\n",
    "    \n",
    "        if len(metadata_df) >= MAX_TOTAL_IMAGES:\n",
    "            break\n",
    "    \n",
    "        time.sleep(1)\n",
    "    \n",
    "    print(f\" Finished progressive sampling. Total images: {len(metadata_df)}\")\n",
    "    \n",
    "    # ==============================\n",
    "    # 6 Cleanup local CSV\n",
    "    # ==============================\n",
    "    if os.path.exists(METADATA_LOCAL):\n",
    "        os.remove(METADATA_LOCAL)\n",
    "        print(f\"ðŸ—‘ï¸ Removed local metadata file: {METADATA_LOCAL}\")\n",
    "    \n",
    "process_landing_zone();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66845993-df05-4cce-9d13-8ee60e1f4d9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting Hugging Face datasets cache at C:\\Users\\Arman\\.cache\\huggingface\\datasets ...\n",
      "  Renamed cache folder to C:\\Users\\Arman\\AppData\\Local\\Temp\\hf_cache_to_delete_1761522365\n",
      " Scheduled background cache deletion.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import tempfile\n",
    "import subprocess\n",
    "import time\n",
    "from datasets import config\n",
    "\n",
    "\"\"\"\n",
    "  Here we notice that getting data from huggingface stores heavy size of cache of that dataset on local storage.\n",
    "  so we try to safely remove this cache and free up space.\n",
    "\"\"\"\n",
    "\n",
    "hf_cache = config.HF_DATASETS_CACHE\n",
    "\n",
    "def delete_cache(path):\n",
    "    # Rename and delete the Hugging Face cache in a background process.\n",
    "    # as windows denies access and stops deleting the cache, it must run in a background process.\n",
    "    if not os.path.exists(path):\n",
    "        print(\" No Hugging Face cache found â€” skipping.\")\n",
    "        return\n",
    "\n",
    "    temp_parent = tempfile.gettempdir()\n",
    "    temp_target = os.path.join(temp_parent, f\"hf_cache_to_delete_{int(time.time())}\")\n",
    "\n",
    "    try:\n",
    "        os.rename(path, temp_target)\n",
    "        print(f\"  Renamed cache folder to {temp_target}\")\n",
    "    except Exception as e:\n",
    "        print(f\" Could not rename cache folder: {e}\")\n",
    "        return\n",
    "\n",
    "    # Launch background deletion process, after 3 seconds of wait\n",
    "    delete_cmd = f'timeout /T 3 >NUL & rmdir /S /Q \"{temp_target}\"'\n",
    "    try:\n",
    "        subprocess.Popen([\"cmd\", \"/c\", delete_cmd], creationflags=subprocess.DETACHED_PROCESS)\n",
    "        print(\" Scheduled background cache deletion.\")\n",
    "    except Exception as e:\n",
    "        print(f\" Could not schedule background deletion: {e}\")\n",
    "\n",
    "# Run cleanup\n",
    "print(f\"Deleting Hugging Face datasets cache at {hf_cache} ...\")\n",
    "delete_cache(hf_cache)\n",
    "\n",
    "# However some binary files from online huggingface hub stay in locat storage\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bd922f-3b00-46bb-bbdd-d0001fbde1a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
