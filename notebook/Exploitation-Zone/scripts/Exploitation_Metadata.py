#!/usr/bin/env python
# coding: utf-8

# In[4]:


"""
-Arman Bazarchi-
Exploitation Zone — Metadata notebook

 - Read trusted metadata CSV(s) from trusted-zone
 - merges values of needed columns of a row into a single string for embedding
 - we keep up to kingdom because model can be used to hold and integrate data of different kingdoms
 - (Animalia, Plante, Fungi)
 - Create text embeddings for text metadata using ChromaDB
 - Avoid duplicates by checking existing uuids in the collection
 - Store embeddings persistently for similarity search
 - we store them in a chroma directory 'exploitation_db' in the 'metadata_embeddings' collection.
"""


from minio import Minio
import pandas as pd
import chromadb
from chromadb.utils import embedding_functions
import os, io
from datetime import datetime

# -----------------------
# 1. Configuration
# -----------------------
def process_exploitation_metadata(
    minio_endpoint = "localhost:9000",
    access_key = "admin",
    secret_key = "password123"):
    """Main function to process exploitation metadata"""
    
    # Setup configuration
    config = _setup_metadata_config()
    
    # Connect to MinIO and validate
    client = _connect_to_minio_metadata(minio_endpoint, access_key, secret_key)
    
    # Load and process metadata
    metadata_df = _load_and_process_metadata(client)
    
    # Setup ChromaDB
    collection, text_embedder = _setup_chromadb_metadata(config)
    
    # Get existing IDs and filter new records
    existing_count, new_df = _get_existing_and_filter_new(collection, metadata_df)
    
    # Process new records if any
    if not new_df.empty:
        _process_new_metadata(collection, new_df, text_embedder, config['COLLECTION_NAME'])
    
    # Display summary
    _display_metadata_summary(collection, existing_count, config['COLLECTION_NAME'])

def _setup_metadata_config():
    """Setup configuration and paths for metadata processing"""
    try:
        SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__)) # in orchestrated
    except NameError:
        SCRIPT_DIR = os.getcwd() # in notebook

    return {
        'SCRIPT_DIR': SCRIPT_DIR,
        'CHROMA_DB': os.path.join(SCRIPT_DIR, "../exploitation_db"),
        'COLLECTION_NAME': "metadata_embeddings",
        'TRUSTED_ZONE': "trusted-zone",
        'TRUSTED_META_PREFIX': "metadata/"
    }

def _connect_to_minio_metadata(minio_endpoint, access_key, secret_key):
    """Connect to MinIO and validate trusted zone exists"""
    client = Minio(minio_endpoint, access_key=access_key, secret_key=secret_key, secure=False)
    
    if not client.bucket_exists("trusted-zone"):
        raise SystemExit(" Trusted zone bucket 'trusted-zone' does not exist. Cannot continue.")
    
    return client

def _load_and_process_metadata(client):
    """Load metadata from MinIO and process it for embedding"""
    # List all metadata CSVs from trusted-zone
    metadata_objs = [
        obj.object_name for obj in client.list_objects("trusted-zone", prefix="metadata/", recursive=True)
        if obj.object_name.lower().endswith(".csv")]

    # raise error if no metadata available
    if not metadata_objs:
        raise SystemExit(" No trusted metadata files found in trusted-zone.")

    # Use the latest trusted metadata
    metadata_objs.sort(reverse=True)
    latest_meta = metadata_objs[0]
    print(f" Loading trusted metadata: {latest_meta}")

    # Download to memory
    resp = client.get_object("trusted-zone", latest_meta)
    data = resp.read()
    resp.close()
    resp.release_conn()
    metadata_df = pd.read_csv(io.BytesIO(data))
    print(f" Loaded trusted metadata with {len(metadata_df)} rows.")

    # Combine text columns for embedding
    text_cols = ["kingdom", "phylum", "class", "order", "family", "genus", "species", "scientific_name", "common"]
    metadata_df["combined_text"] = metadata_df[text_cols].fillna("").agg(" ".join, axis=1).str.strip()

    # Drop rows without valid UUID or text
    metadata_df = metadata_df.dropna(subset=["uuid", "combined_text"])
    metadata_df = metadata_df[metadata_df["combined_text"].str.len() > 0]
    print(f" Cleaned metadata for embedding: {len(metadata_df)} valid rows.")
    
    return metadata_df

def _setup_chromadb_metadata(config):
    """Setup ChromaDB client and collection"""
    chroma_client = chromadb.PersistentClient(path=config['CHROMA_DB'])

    # Create or load the collection
    collection = chroma_client.get_or_create_collection(
        name=config['COLLECTION_NAME'],
        metadata={"description": "Embeddings for trusted metadata records"}
    )

    # Use default text embedding function
    text_embedder = embedding_functions.DefaultEmbeddingFunction()
    
    return collection, text_embedder

def _get_existing_and_filter_new(collection, metadata_df):
    """Get existing IDs and filter new records"""
    existing_count = collection.count()

    # Get all existing UUIDs here stored as 'ids' (if collection is not empty)
    existing_ids = []
    if existing_count > 0:
        batch_size = 500
        offset = 0
        while True:
            batch = collection.get(limit=batch_size, offset=offset)
            if not batch["ids"]:
                break
            existing_ids.extend(batch["ids"])
            offset += batch_size

    existing_ids = set(existing_ids)
    print(f" Existing embeddings in collection: {len(existing_ids)}")

    # Filter new rows
    new_df = metadata_df[~metadata_df["uuid"].isin(existing_ids)]
    print(f" New records to embed: {len(new_df)}")
    
    return existing_count, new_df

def _process_new_metadata(collection, new_df, text_embedder, collection_name):
    """Process new metadata records and add to collection"""
    texts = new_df["combined_text"].tolist()
    uuids = new_df["uuid"].tolist()

    print(" Generating embeddings...")
    embeddings = text_embedder(texts)

    # setting a batch size to avoid exceeding the maximum allowed limit(5461)
    batch_size = 5000
    total = len(uuids)
    start_idx = 0

    print(f" Adding {total} new embeddings to collection '{collection_name}' in batches...")

    while start_idx < total:
        end_idx = min(start_idx + batch_size, total)

        try:
            collection.add(
                ids=uuids[start_idx:end_idx],
                embeddings=embeddings[start_idx:end_idx],
                metadatas=new_df.iloc[start_idx:end_idx].to_dict(orient="records"),
                documents=texts[start_idx:end_idx],
            )
            print(f"  Added records {start_idx + 1}-{end_idx} / {total}")
            start_idx = end_idx  # move to next batch

        except Exception as e:
            print(f"   Batch {start_idx + 1}-{end_idx} failed: {e}")

            # If batch too big, reduce size and retry
            if "max batch size" in str(e).lower() and batch_size > 1000:
                old_batch = batch_size
                batch_size = batch_size // 2
                print(f"  Reducing batch size from {old_batch} → {batch_size} and retrying...")
            else:
                raise

    print(f"  Added {total} new embeddings to collection '{collection_name}'.")

def _display_metadata_summary(collection, existing_count, collection_name):
    """Display processing summary"""
    final_count = collection.count()
    added_count = final_count - existing_count

    print("\n ===== Summary =====")
    print(f" Collection: {collection_name}")
    print(f" Previously had: {existing_count}")
    print(f" New added: {added_count}")
    print(f" Total now: {final_count}")
    print("=======================")
    print(" Exploitation Metadata processing complete.")


if __name__ == "__main__":
    process_exploitation_metadata()


# In[ ]:




