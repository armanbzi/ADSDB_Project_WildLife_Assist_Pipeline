{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25935188-3c40-45fa-8b0e-64c6a74cd1aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter the following parameters:\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "MAX_PER_SPECIES (e.g., 30):  1\n",
      "MAX_SPECIES_PER_FAMILY (e.g., 11):  1\n",
      "MAX_SAMPLES (e.g., 300000):  1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded metadata from temporal-landing with 2 records.\n",
      " Scanning existing images in temporal-landing...\n",
      " Found 2 existing image files in temporal-landing.\n",
      " Tracking 2 families from existing metadata.\n",
      " Processing streaming data...\n",
      " Processing up to 1000 samples from the dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1000it [00:07, 139.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Finished progressive sampling. Total images: 2\n",
      " Removed local metadata file: metadata_final.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "-Arman Bazarchi-\n",
    "Temporal_Landing\n",
    "here we get the raw data(image and metadata about each image) \n",
    "from source (imageomics/TreeOfLife-200M) and store in our temporal-zone.\n",
    "asks user for minIO configurations.\n",
    "as data is huge 200m, we choose streaming it rather than storing heavy data in local storage and then process.\n",
    "and we define a maximum samples for each run to define how many samples of the main dataset(200m) to iterate.\n",
    "user is asked for a max samples, maximum number of species per family, limiting data to store up to a defined  \n",
    "number of species for each stored family.\n",
    "user also is asked for a maximum number of observations(images) per each stored species.\n",
    "for now only retrieves images of snake families only because of not being able to store a lot of data at the moment.\n",
    "\n",
    "then code connects to minIO create a temporal-zone bucket and a subbucket 'temporal-landing', \n",
    "stores images in a folder 'images' and a csv file of the needed metadata in a folder 'metadata',\n",
    "it checks and avoids storing duplicates in temporal-zone, \n",
    "\n",
    "we use streaming mode to retrieve data we need, because data is huge and is not best option\n",
    "to store in local storage and then filter to store\n",
    "\n",
    "# Login using e.g. `huggingface-cli login` to access this dataset\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from datasets import load_dataset\n",
    "from datasets import config\n",
    "import shutil\n",
    "from minio import Minio\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import io\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import requests\n",
    "import re\n",
    "\n",
    "# ==============================\n",
    "#          Functions\n",
    "# ==============================\n",
    "\n",
    "def get_user_parameters():\n",
    "    # Get user input parameters for data processing with validation control.\n",
    "    \n",
    "    print(\"Please enter the following parameters:\")\n",
    "    \n",
    "    # Get MAX_PER_SPECIES with validation\n",
    "    while True:\n",
    "        try:\n",
    "            max_per_species = int(input(\"MAX_PER_SPECIES (e.g., 30): \"))\n",
    "            if max_per_species > 0:\n",
    "                break\n",
    "            else:\n",
    "                print(\" Please enter a positive number greater than 0.\")\n",
    "        except ValueError:\n",
    "            print(\" Please enter a valid number.\")\n",
    "    \n",
    "    # Get MAX_SPECIES_PER_FAMILY with validation\n",
    "    while True:\n",
    "        try:\n",
    "            max_species_per_family = int(input(\"MAX_SPECIES_PER_FAMILY (e.g., 11): \"))\n",
    "            if max_species_per_family > 0:\n",
    "                break\n",
    "            else:\n",
    "                print(\" Please enter a positive number greater than 0.\")\n",
    "        except ValueError:\n",
    "            print(\" Please enter a valid number.\")\n",
    "    \n",
    "    # Get MAX_SAMPLES with validation\n",
    "    while True:\n",
    "        try:\n",
    "            max_samples = int(input(\"MAX_SAMPLES (e.g., 300000): \"))\n",
    "            if max_samples > 0:\n",
    "                break\n",
    "            else:\n",
    "                print(\" Please enter a positive number greater than 0.\")\n",
    "        except ValueError:\n",
    "            print(\" Please enter a valid number.\")\n",
    "    \n",
    "    return max_per_species, max_species_per_family, max_samples\n",
    "\n",
    "def setup_minio_buckets(client, root_bucket, temp_prefix):\n",
    "    # Setup MinIO buckets and subfolders.\n",
    "    \n",
    "    # Ensure temporal-landing and temporal-zone exist\n",
    "    if not client.bucket_exists(root_bucket):\n",
    "        client.make_bucket(root_bucket)\n",
    "        print(f\" Created top-level bucket: {root_bucket}\")\n",
    "    \n",
    "    # Create temporal-landing subfolder if missing\n",
    "    temporal_exists = any(\n",
    "        obj.object_name.startswith(f\"{temp_prefix}/\")\n",
    "        for obj in client.list_objects(root_bucket, recursive=False)\n",
    "    )\n",
    "    if not temporal_exists:\n",
    "        client.put_object(\n",
    "            root_bucket,\n",
    "            f\"{temp_prefix}/.init\",\n",
    "            data=io.BytesIO(b\"init\"),\n",
    "            length=4,\n",
    "            content_type=\"text/plain\"\n",
    "        )\n",
    "        print(f\" Created subfolder: {temp_prefix}/ inside {root_bucket}\")\n",
    "\n",
    "def load_existing_metadata(client, root_bucket, metadata_remote_path, metadata_local):\n",
    "    # Load existing metadata or create empty DataFrame if no existing available.\n",
    "    \n",
    "    try:\n",
    "        client.fget_object(root_bucket, metadata_remote_path, metadata_local)\n",
    "        metadata_df = pd.read_csv(metadata_local)\n",
    "        print(f\" Loaded metadata from temporal-landing with {len(metadata_df)} records.\")\n",
    "    except Exception:\n",
    "        metadata_df = pd.DataFrame(columns=[\n",
    "            \"uuid\",\"temporal_path\", \"kingdom\", \"phylum\", \"class\", \"order\", \"family\",\n",
    "            \"genus\", \"species\", \"scientific_name\", \"common\", \"image_url\"\n",
    "        ])\n",
    "        print(\" No metadata found in temporal-landing â€” starting fresh.\")\n",
    "    \n",
    "    # Maintain fast lookup sets\n",
    "    existing_uuids = set(metadata_df[\"uuid\"].dropna().unique()) if \"uuid\" in metadata_df.columns else set()\n",
    "    return metadata_df, existing_uuids\n",
    "\n",
    "def scan_existing_images(client, root_bucket, temp_prefix):\n",
    "    # Scan existing images in temporal-landing.\n",
    "    \n",
    "    print(\" Scanning existing images in temporal-landing...\")\n",
    "    \n",
    "    existing_image_ids = set()\n",
    "    for obj in client.list_objects(root_bucket, prefix=f\"{temp_prefix}/images/\", recursive=True):\n",
    "        # Extract UUID from filename: temporal-landing/images/<uuid>.jpg\n",
    "        match = re.match(rf\"{temp_prefix}/images/([a-f0-9\\-]+)\\.jpg\", obj.object_name, re.IGNORECASE)\n",
    "        if match:\n",
    "            existing_image_ids.add(match.group(1))\n",
    "    \n",
    "    print(f\" Found {len(existing_image_ids)} existing image files in temporal-landing.\")\n",
    "    return existing_image_ids\n",
    "\n",
    "def build_family_species_map(metadata_df):\n",
    "    # Build family -> species map from metadata.\n",
    "    family_species = {}\n",
    "    species_counts = {}\n",
    "    \n",
    "    if not metadata_df.empty:\n",
    "        for _, row in metadata_df.iterrows():\n",
    "            fam = row.get(\"family\")\n",
    "            sp = row.get(\"species\")\n",
    "            if pd.notna(fam) and pd.notna(sp):\n",
    "                family_species.setdefault(fam, set()).add(sp)\n",
    "                species_counts[sp] = species_counts.get(sp, 0) + 1\n",
    "    \n",
    "    print(f\" Tracking {len(family_species)} families from existing metadata.\")\n",
    "    return family_species, species_counts\n",
    "\n",
    "def is_snake_family(sample, snake_families):\n",
    "    # Check if sample belongs to snake families.\n",
    "    \n",
    "    kingdom = sample.get(\"kingdom\")\n",
    "    cls = sample.get(\"class\")\n",
    "    family = sample.get(\"family\") or \"unknown\"\n",
    "    \n",
    "    return (\n",
    "        kingdom and kingdom.strip().lower() == \"animalia\"\n",
    "        and cls and cls.strip().lower() == \"squamata\"\n",
    "        and family in snake_families\n",
    "    )\n",
    "\n",
    "def skip_species(species, family, species_counts, family_species, max_per_species, max_species_per_family):\n",
    "    # Check if species should be skipped based on limits.\n",
    "    \n",
    "    # Skip if already have enough per this species\n",
    "    count = species_counts.get(species, 0)\n",
    "    if count >= max_per_species:\n",
    "        return True\n",
    "    \n",
    "    # Skip if already have enough species for this family\n",
    "    current_species_in_family = family_species.get(family, set())\n",
    "    if len(current_species_in_family) >= max_species_per_family and species not in current_species_in_family:\n",
    "        return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "def download_and_save_image(image_url, img_id, client, root_bucket, temp_prefix, existing_image_ids):\n",
    "    # Download and save image to MinIO.\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(image_url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        image_bytes = response.content\n",
    "        \n",
    "        img = Image.open(io.BytesIO(image_bytes))\n",
    "        img.verify()\n",
    "        buffer = io.BytesIO(image_bytes)\n",
    "        \n",
    "        # Save to MinIO \n",
    "        object_name = f\"{temp_prefix}/images/{img_id}.jpg\"\n",
    "        client.put_object(\n",
    "            root_bucket,\n",
    "            object_name,\n",
    "            data=buffer,\n",
    "            length=len(image_bytes),\n",
    "            content_type=\"image/jpeg\"\n",
    "        )\n",
    "        existing_image_ids.add(img_id)\n",
    "        return object_name\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Error downloading image: {e}\")\n",
    "\n",
    "def save_metadata_record(sample, img_id, object_name, metadata_df, metadata_local, minio_config, existing_uuids, species_counts, family_species, species, family, limits):\n",
    "    # Save metadata record to CSV and MinIO.\n",
    "    \n",
    "    if img_id not in existing_uuids:\n",
    "        # Save needed columns\n",
    "        record = {\n",
    "            \"uuid\": img_id,\n",
    "            \"temporal_path\": object_name,\n",
    "            \"kingdom\": sample.get(\"kingdom\"),\n",
    "            \"phylum\": sample.get(\"phylum\"),\n",
    "            \"class\": sample.get(\"class\"),\n",
    "            \"order\": sample.get(\"order\"),\n",
    "            \"family\": family,\n",
    "            \"genus\": sample.get(\"genus\"),\n",
    "            \"species\": species,\n",
    "            \"scientific_name\": sample.get(\"scientific_name\"),\n",
    "            \"common\": sample.get(\"common\"),\n",
    "            \"image_url\": sample.get(\"source_url\")\n",
    "        }\n",
    "        \n",
    "        metadata_df = pd.concat([metadata_df, pd.DataFrame([record])], ignore_index=True)\n",
    "        metadata_df.to_csv(metadata_local, index=False)\n",
    "        minio_config['client'].fput_object(minio_config['root_bucket'], minio_config['metadata_remote_path'], metadata_local, content_type=\"text/csv\")\n",
    "        \n",
    "        species_counts[species] = species_counts.get(species, 0) + 1\n",
    "        family_species.setdefault(family, set()).add(species)\n",
    "        existing_uuids.add(img_id)\n",
    "        \n",
    "        print(f\" Saved {species} ({species_counts[species]}/{limits['max_per_species']}) \"\n",
    "              f\"in family {family} ({len(family_species.get(family, []))}/{limits['max_species_per_family']})\")\n",
    "    \n",
    "    return metadata_df\n",
    "\n",
    "def process_sample(sample, snake_families, limits, client, root_bucket, temp_prefix, \n",
    "                   existing_image_ids, existing_uuids, metadata_df, species_counts, family_species, \n",
    "                   metadata_local, metadata_remote_path):\n",
    "    \n",
    "    # Process a single sample and return updated metadata_df.\n",
    "    species = sample.get(\"species\") or \"unknown\"\n",
    "    family = sample.get(\"family\") or \"unknown\"\n",
    "    img_id = sample.get(\"uuid\")\n",
    "    image_url = sample.get(\"source_url\")\n",
    "    \n",
    "    # Only save snake images for now\n",
    "    if not is_snake_family(sample, snake_families):\n",
    "        return metadata_df\n",
    "    \n",
    "    # Skip if limits exceeded\n",
    "    if skip_species(species, family, species_counts, family_species, limits['max_per_species'], limits['max_species_per_family']):\n",
    "        return metadata_df\n",
    "    \n",
    "    try:\n",
    "        object_name = None\n",
    "        if img_id not in existing_image_ids:\n",
    "            object_name = download_and_save_image(image_url, img_id, client, root_bucket, temp_prefix, existing_image_ids)\n",
    "        \n",
    "        # Save metadata\n",
    "        minio_config = {\n",
    "            'client': client,\n",
    "            'root_bucket': root_bucket,\n",
    "            'metadata_remote_path': metadata_remote_path\n",
    "        }\n",
    "        metadata_df = save_metadata_record(\n",
    "            sample, img_id, object_name, metadata_df, metadata_local, \n",
    "            minio_config, existing_uuids, species_counts, family_species, \n",
    "            species, family, limits)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\" Error saving {species}: {e}\")\n",
    "    \n",
    "    return metadata_df\n",
    "\n",
    "def process_streaming_data(snake_families, limits, max_samples, client, root_bucket, temp_prefix, existing_image_ids, existing_uuids, metadata_df, species_counts, family_species, metadata_local, metadata_remote_path):\n",
    "    # Process streaming data with fixed limit.\n",
    "    # connecting to dataset through imagomics to get raw data.\n",
    "    \n",
    "    print(\" Processing streaming data...\")\n",
    "    \n",
    "    try:\n",
    "        # Stream the dataset (no full download)\n",
    "        dataset = load_dataset(\n",
    "            \"imageomics/TreeOfLife-200M\",\n",
    "            split=\"train\",\n",
    "            streaming=True\n",
    "        )\n",
    "        \n",
    "        # Use user-provided max_samples limit\n",
    "        print(f\" Processing up to {max_samples} samples from the dataset...\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\" Error loading dataset: {e}\")\n",
    "        return metadata_df, False\n",
    "    \n",
    "    # Process samples in stream\n",
    "    for i, sample in enumerate(tqdm(dataset)):\n",
    "        # Stop after processing max_samples\n",
    "        if i >= max_samples:\n",
    "            break\n",
    "        \n",
    "        metadata_df = process_sample(\n",
    "            sample, snake_families, limits, client, root_bucket, temp_prefix, \n",
    "            existing_image_ids, existing_uuids, metadata_df, species_counts, \n",
    "            family_species, metadata_local, metadata_remote_path\n",
    "        )\n",
    "    \n",
    "    return metadata_df, False\n",
    "\n",
    "def cleanup_local_files(metadata_local):\n",
    "    \"\"\"Clean up local CSV file.\"\"\"\n",
    "    if os.path.exists(metadata_local):\n",
    "        os.remove(metadata_local)\n",
    "        print(f\" Removed local metadata file: {metadata_local}\")\n",
    "\n",
    "# ==============================\n",
    "#        Configuration\n",
    "# ==============================\n",
    "def process_temporal(\n",
    "    minio_endpoint = \"localhost:9000\",\n",
    "    access_key = \"admin\",\n",
    "    secret_key = \"password123\"):\n",
    "\n",
    "    ROOT_BUCKET = \"temporal-zone\"         \n",
    "    TEMP_PREFIX = \"temporal-landing\"     # Subbucket for temporal\n",
    "    \n",
    "    # Metadata config\n",
    "    METADATA_REMOTE_PATH = f\"{TEMP_PREFIX}/metadata/metadata_final.csv\"\n",
    "    METADATA_LOCAL = \"metadata_final.csv\"\n",
    "    \n",
    "    # Get user parameters\n",
    "    MAX_PER_SPECIES, MAX_SPECIES_PER_FAMILY, MAX_SAMPLES = get_user_parameters()\n",
    "    \n",
    "\n",
    "    \n",
    "    # Known snake families\n",
    "    SNAKE_FAMILIES = {\n",
    "        \"Viperidae\", \"Elapidae\", \"Colubridae\", \"Pythonidae\", \"Boidae\",\n",
    "        \"Typhlopidae\", \"Lamprophiidae\", \"Natricidae\", \"Dipsadidae\",\n",
    "        \"Leptotyphlopidae\", \"Xenopeltidae\", \"Anomalepididae\", \"Loxocemidae\",\n",
    "        \"Uropeltidae\", \"Cylindrophiidae\", \"Aniliidae\", \"Acrochhordidae\",\n",
    "        \"Anomochilidae\", \"Atractaspididae\", \"Bolyeridae\"\n",
    "    }\n",
    "    \n",
    "    # Initialize MinIO client\n",
    "    client = Minio(minio_endpoint, access_key=access_key, secret_key=secret_key, secure=False)\n",
    "    \n",
    "    # Setup buckets\n",
    "    setup_minio_buckets(client, ROOT_BUCKET, TEMP_PREFIX)\n",
    "    \n",
    "    # Load existing metadata\n",
    "    metadata_df, existing_uuids = load_existing_metadata(client, ROOT_BUCKET, METADATA_REMOTE_PATH, METADATA_LOCAL)\n",
    "    \n",
    "    # Scan existing images\n",
    "    existing_image_ids = scan_existing_images(client, ROOT_BUCKET, TEMP_PREFIX)\n",
    "    \n",
    "    # Build family species map\n",
    "    family_species, species_counts = build_family_species_map(metadata_df)\n",
    "    \n",
    "    # Process streaming data\n",
    "    limits = {'max_per_species': MAX_PER_SPECIES, 'max_species_per_family': MAX_SPECIES_PER_FAMILY}\n",
    "    metadata_df, _ = process_streaming_data(\n",
    "        SNAKE_FAMILIES, limits, MAX_SAMPLES,\n",
    "        client, ROOT_BUCKET, TEMP_PREFIX, existing_image_ids, existing_uuids,\n",
    "        metadata_df, species_counts, family_species, METADATA_LOCAL, METADATA_REMOTE_PATH)\n",
    "    \n",
    "    print(f\" Finished progressive sampling. Total images: {len(metadata_df)}\")\n",
    "    \n",
    "    # Cleanup\n",
    "    cleanup_local_files(METADATA_LOCAL)\n",
    "\n",
    "process_temporal();\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bd922f-3b00-46bb-bbdd-d0001fbde1a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
