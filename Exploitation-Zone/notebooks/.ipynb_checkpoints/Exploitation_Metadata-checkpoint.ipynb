{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7efffba4-8f04-497e-ad59-661dfd733e39",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loading trusted metadata: metadata/trusted_metadata_2025_10_29_07_18_26.csv\n",
      " Loaded trusted metadata with 0 rows.\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "['combined_text']",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[32m~\\AppData\\Local\\Temp\\ipykernel_20544\\2784824847.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    178\u001b[39m     print(f\" Total now: {final_count}\")\n\u001b[32m    179\u001b[39m     print(\u001b[33m\"=======================\"\u001b[39m)\n\u001b[32m    180\u001b[39m     print(\u001b[33m\" Exploitation Metadata processing complete.\"\u001b[39m)\n\u001b[32m    181\u001b[39m \n\u001b[32m--> \u001b[39m\u001b[32m182\u001b[39m process_exploitation_metadata();\n\u001b[32m    183\u001b[39m a\n",
      "\u001b[32m~\\AppData\\Local\\Temp\\ipykernel_20544\\2784824847.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(MINIO, ACCESS_KEY, SECRET_KEY)\u001b[39m\n\u001b[32m     79\u001b[39m         metadata_df[\u001b[33m\"combined_text\"\u001b[39m] = metadata_df[text_cols].applymap(\u001b[38;5;28;01mlambda\u001b[39;00m x: str(x) \u001b[38;5;28;01mif\u001b[39;00m pd.notna(x) \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\"\u001b[39m).agg(\u001b[33m\" \"\u001b[39m.join, axis=\u001b[32m1\u001b[39m).str.strip()\n\u001b[32m     80\u001b[39m \n\u001b[32m     81\u001b[39m \n\u001b[32m     82\u001b[39m     \u001b[38;5;66;03m# Drop rows without valid UUID or text\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m83\u001b[39m     metadata_df = metadata_df.dropna(subset=[\u001b[33m\"uuid\"\u001b[39m, \u001b[33m\"combined_text\"\u001b[39m])\n\u001b[32m     84\u001b[39m     metadata_df = metadata_df[metadata_df[\u001b[33m\"combined_text\"\u001b[39m].str.len() > \u001b[32m0\u001b[39m]\n\u001b[32m     85\u001b[39m     print(f\" Cleaned metadata for embedding: {len(metadata_df)} valid rows.\")\n\u001b[32m     86\u001b[39m \n",
      "\u001b[32mC:\\Python312\\Lib\\site-packages\\pandas\\core\\frame.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, axis, how, thresh, subset, inplace, ignore_index)\u001b[39m\n\u001b[32m   6673\u001b[39m             ax = self._get_axis(agg_axis)\n\u001b[32m   6674\u001b[39m             indices = ax.get_indexer_for(subset)\n\u001b[32m   6675\u001b[39m             check = indices == -\u001b[32m1\u001b[39m\n\u001b[32m   6676\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m check.any():\n\u001b[32m-> \u001b[39m\u001b[32m6677\u001b[39m                 \u001b[38;5;28;01mraise\u001b[39;00m KeyError(np.array(subset)[check].tolist())\n\u001b[32m   6678\u001b[39m             agg_obj = self.take(indices, axis=agg_axis)\n\u001b[32m   6679\u001b[39m \n\u001b[32m   6680\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m thresh \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m lib.no_default:\n",
      "\u001b[31mKeyError\u001b[39m: ['combined_text']"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "-Arman Bazarchi-\n",
    "Exploitation Zone — Metadata notebook\n",
    "\n",
    " - Read trusted metadata CSV(s) from trusted-zone\n",
    " - merges values of needed columns of a row into a single string for embedding\n",
    " - we keep up to kingdom because model can be used to hold and integrate data of different kingdoms\n",
    " - (Animalia, Plante, Fungi)\n",
    " - Create text embeddings for text metadata using ChromaDB\n",
    " - Avoid duplicates by checking existing uuids in the collection\n",
    " - Store embeddings persistently for similarity search\n",
    " - we store them in a chroma directory 'exploitation_db' in the 'metadata_embeddings' collection.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from minio import Minio\n",
    "import pandas as pd\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "import os, io\n",
    "from datetime import datetime\n",
    "\n",
    "# -----------------------\n",
    "#      Configuration\n",
    "# -----------------------\n",
    "def process_exploitation_metadata(\n",
    "    MINIO = \"localhost:9000\",\n",
    "    ACCESS_KEY = \"admin\",\n",
    "    SECRET_KEY = \"password123\"):\n",
    "\n",
    "    TRUSTED_ZONE = \"trusted-zone\"\n",
    "    TRUSTED_META_PREFIX = \"metadata/\"\n",
    "    \n",
    "    # set the working directory\n",
    "    try:\n",
    "        SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__)) # in orchestrated\n",
    "    except NameError:\n",
    "        SCRIPT_DIR = os.getcwd() # in notebook\n",
    "        \n",
    "    CHROMA_DB = os.path.join(SCRIPT_DIR, \"../exploitation_db\")\n",
    "\n",
    "    COLLECTION_NAME = \"metadata_embeddings\"    \n",
    "    \n",
    "    #  Connect to MinIO\n",
    "    client = Minio(MINIO, access_key=ACCESS_KEY, secret_key=SECRET_KEY, secure=False)\n",
    "    \n",
    "    # raise error if no trusted-zone \n",
    "    if not client.bucket_exists(TRUSTED_ZONE):\n",
    "        raise SystemExit(f\" Trusted zone bucket '{TRUSTED_ZONE}' does not exist. Cannot continue.\")\n",
    "    \n",
    "    \n",
    "    # List all metadata CSVs from trusted-zone\n",
    "    metadata_objs = [\n",
    "        obj.object_name for obj in client.list_objects(TRUSTED_ZONE, prefix=TRUSTED_META_PREFIX, recursive=True)\n",
    "        if obj.object_name.lower().endswith(\".csv\")]\n",
    "    \n",
    "    # raise error if no metadata available\n",
    "    if not metadata_objs:\n",
    "        raise SystemExit(\" No trusted metadata files found in trusted-zone.\")\n",
    "    \n",
    "    # Use the latest trusted metadata\n",
    "    metadata_objs.sort(reverse=True)\n",
    "    latest_meta = metadata_objs[0]\n",
    "    print(f\" Loading trusted metadata: {latest_meta}\")\n",
    "    \n",
    "    # Download to memory\n",
    "    resp = client.get_object(TRUSTED_ZONE, latest_meta)\n",
    "    data = resp.read()\n",
    "    resp.close()\n",
    "    resp.release_conn()\n",
    "    metadata_df = pd.read_csv(io.BytesIO(data))\n",
    "    print(f\" Loaded trusted metadata with {len(metadata_df)} rows.\")\n",
    "    \n",
    "    # -----------------------\n",
    "    #   Combine text columns for embedding\n",
    "    # -----------------------\n",
    "    text_cols = [\"kingdom\", \"phylum\", \"class\", \"order\", \"family\", \"genus\", \"species\", \"scientific_name\", \"common\"]\n",
    "    \n",
    "    if not metadata_df.empty:\n",
    "        # Ensure all text columns exist\n",
    "        for col in text_cols:\n",
    "            if col not in metadata_df.columns:\n",
    "                metadata_df[col] = \"\"\n",
    "    \n",
    "        # Combine text for embedding\n",
    "        metadata_df[\"combined_text\"] = metadata_df[text_cols].applymap(lambda x: str(x) if pd.notna(x) else \"\").agg(\" \".join, axis=1).str.strip()\n",
    "\n",
    "    \n",
    "    # Drop rows without valid UUID or text\n",
    "    metadata_df = metadata_df.dropna(subset=[\"uuid\", \"combined_text\"])\n",
    "    metadata_df = metadata_df[metadata_df[\"combined_text\"].str.len() > 0]\n",
    "    print(f\" Cleaned metadata for embedding: {len(metadata_df)} valid rows.\")\n",
    "    \n",
    "    \n",
    "    #  Connect to ChromaDB \n",
    "    chroma_client = chromadb.PersistentClient(path=CHROMA_DB)\n",
    "    \n",
    "    # Create or load the collection\n",
    "    collection = chroma_client.get_or_create_collection(\n",
    "        name=COLLECTION_NAME,\n",
    "        metadata={\"description\": \"Embeddings for trusted metadata records\"}\n",
    "    )\n",
    "    \n",
    "    # Use default text embedding function\n",
    "    text_embedder = embedding_functions.DefaultEmbeddingFunction()\n",
    "    \n",
    "    # -----------------------\n",
    "    #  Avoid duplicates — check existing UUIDs\n",
    "    # -----------------------\n",
    "    existing_count = collection.count()\n",
    "    \n",
    "    # Get all existing UUIDs here stored as 'ids' (if collection is not empty)\n",
    "    existing_ids = []\n",
    "    if existing_count > 0:\n",
    "        batch_size = 500\n",
    "        offset = 0\n",
    "        while True:\n",
    "            batch = collection.get(limit=batch_size, offset=offset)\n",
    "            if not batch[\"ids\"]:\n",
    "                break\n",
    "            existing_ids.extend(batch[\"ids\"])\n",
    "            offset += batch_size\n",
    "    \n",
    "    existing_ids = set(existing_ids)\n",
    "    print(f\" Existing embeddings in collection: {len(existing_ids)}\")\n",
    "    \n",
    "    # Filter new rows\n",
    "    new_df = metadata_df[~metadata_df[\"uuid\"].isin(existing_ids)]\n",
    "    print(f\" New records to embed: {len(new_df)}\")\n",
    "    \n",
    "    # -----------------------\n",
    "    #   Generate and add embeddings\n",
    "    # -----------------------\n",
    "    if new_df.empty:\n",
    "        print(\" No new metadata to embed. No data was added.\")\n",
    "    else:\n",
    "        texts = new_df[\"combined_text\"].tolist()\n",
    "        uuids = new_df[\"uuid\"].tolist()\n",
    "    \n",
    "        print(\" Generating embeddings...\")\n",
    "        embeddings = text_embedder(texts)\n",
    "    \n",
    "        # setting a barch size to avoid exceeding the maximum allowed limit(5461)\n",
    "        batch_size = 5000\n",
    "        total = len(uuids)\n",
    "        start_idx = 0\n",
    "    \n",
    "        print(f\" Adding {total} new embeddings to collection '{COLLECTION_NAME}' in batches...\")\n",
    "    \n",
    "        while start_idx < total:\n",
    "            end_idx = min(start_idx + batch_size, total)\n",
    "    \n",
    "            try:\n",
    "                collection.add(\n",
    "                    ids=uuids[start_idx:end_idx],\n",
    "                    embeddings=embeddings[start_idx:end_idx],\n",
    "                    metadatas=new_df.iloc[start_idx:end_idx].to_dict(orient=\"records\"),\n",
    "                    documents=texts[start_idx:end_idx],\n",
    "                )\n",
    "                print(f\"  Added records {start_idx + 1}-{end_idx} / {total}\")\n",
    "                start_idx = end_idx  # move to next batch\n",
    "    \n",
    "            except Exception as e:\n",
    "                print(f\"   Batch {start_idx + 1}-{end_idx} failed: {e}\")\n",
    "    \n",
    "                # If batch too big, reduce size and retry\n",
    "                if \"max batch size\" in str(e).lower() and batch_size > 1000:\n",
    "                    old_batch = batch_size\n",
    "                    batch_size = batch_size // 2\n",
    "                    print(f\"  Reducing batch size from {old_batch} → {batch_size} and retrying...\")\n",
    "                else:\n",
    "                    raise\n",
    "    \n",
    "        print(f\"  Added {total} new embeddings to collection '{COLLECTION_NAME}'.\")\n",
    "    \n",
    "    \n",
    "    #  Summary\n",
    "    final_count = collection.count()\n",
    "    added_count = final_count - existing_count\n",
    "    \n",
    "    print(\"\\n ===== Summary =====\")\n",
    "    print(f\" Collection: {COLLECTION_NAME}\")\n",
    "    print(f\" Previously had: {existing_count}\")\n",
    "    print(f\" New added: {added_count}\")\n",
    "    print(f\" Total now: {final_count}\")\n",
    "    print(\"=======================\")\n",
    "    print(\" Exploitation Metadata processing complete.\")\n",
    "\n",
    "process_exploitation_metadata();\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64413288-6e8d-4616-a423-0936aaf06068",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
