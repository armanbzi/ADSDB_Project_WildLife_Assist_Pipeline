"""
-Arman Bazarchi-
Exploitation Zone — Metadata notebook

 - Read trusted metadata CSV(s) from trusted-zone
 - merges values of needed columns of a row into a single string for embedding
 - we keep up to kingdom because model can be used to hold and integrate data of different kingdoms
 - (Animalia, Plante, Fungi)
 - Create text embeddings for text metadata using ChromaDB
 - Avoid duplicates by checking existing uuids in the collection
 - Store embeddings persistently for similarity search
 - we store them in a chroma directory 'exploitation_db' in the 'metadata_embeddings' collection.
"""


from minio import Minio
import pandas as pd
import chromadb
from chromadb.utils import embedding_functions
import os, io
from datetime import datetime

# -----------------------
#      Functions
# -----------------------
def get_minio_config():
    # Load MinIO configuration from environment variables (set by orchestrator).
    
    import os
    
    # Get configuration from environment variables (set by orchestrator)
    endpoint = os.getenv('MINIO_ENDPOINT', 'localhost:9000')
    access_key = os.getenv('MINIO_ACCESS_KEY', 'admin')
    secret_key = os.getenv('MINIO_SECRET_KEY', 'admin123')
    
    print(f"Using MinIO configuration from environment variables: endpoint={endpoint}, access_key={access_key[:3]}***")
    return endpoint, access_key, secret_key

# -----------------------
#      Configuration
# -----------------------
def process_exploitation_metadata(
    MINIO = "localhost:9000",
    ACCESS_KEY = "admin",
    SECRET_KEY = "password123"):

    # Get MinIO configuration from environment variables (set by orchestrator)
    MINIO, ACCESS_KEY, SECRET_KEY = get_minio_config()

    TRUSTED_ZONE = "trusted-zone"
    TRUSTED_META_PREFIX = "metadata/"
    
    # set the working directory
    try:
        SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__)) # in orchestrated
    except NameError:
        SCRIPT_DIR = os.getcwd() # in notebook
        
    CHROMA_DB = os.path.join(SCRIPT_DIR, "../exploitation_db")

    COLLECTION_NAME = "metadata_embeddings"    
    
    #  Connect to MinIO
    client = Minio(MINIO, access_key=ACCESS_KEY, secret_key=SECRET_KEY, secure=False)
    
    # raise error if no trusted-zone 
    if not client.bucket_exists(TRUSTED_ZONE):
        raise SystemExit(f" Trusted zone bucket '{TRUSTED_ZONE}' does not exist. Cannot continue.")
    
    
    # List all metadata CSVs from trusted-zone
    metadata_objs = [
        obj.object_name for obj in client.list_objects(TRUSTED_ZONE, prefix=TRUSTED_META_PREFIX, recursive=True)
        if obj.object_name.lower().endswith(".csv")]
    
    # raise error if no metadata available
    if not metadata_objs:
        raise SystemExit(" No trusted metadata files found in trusted-zone.")
    
    # Use the latest trusted metadata
    metadata_objs.sort(reverse=True)
    latest_meta = metadata_objs[0]
    print(f" Loading trusted metadata: {latest_meta}")
    
    # Download to memory
    resp = client.get_object(TRUSTED_ZONE, latest_meta)
    data = resp.read()
    resp.close()
    resp.release_conn()
    metadata_df = pd.read_csv(io.BytesIO(data))
    print(f" Loaded trusted metadata with {len(metadata_df)} rows.")
    
    # -----------------------
    #   Combine text columns for embedding
    # -----------------------
    if len(metadata_df) == 0:
        print(" No metadata found in trusted zone. Skipping embedding process.")
        return
    
    text_cols = ["kingdom", "phylum", "class", "order", "family", "genus", "species", "scientific_name", "common"]
    metadata_df["combined_text"] = metadata_df[text_cols].fillna("").agg(" ".join, axis=1).str.strip()
    
    # Drop rows without valid UUID or text
    metadata_df = metadata_df.dropna(subset=["uuid", "combined_text"])
    metadata_df = metadata_df[metadata_df["combined_text"].str.len() > 0]
    print(f" Cleaned metadata for embedding: {len(metadata_df)} valid rows.")
    
    
    #  Connect to ChromaDB 
    chroma_client = chromadb.PersistentClient(path=CHROMA_DB)
    
    # Create or load the collection
    collection = chroma_client.get_or_create_collection(
        name=COLLECTION_NAME,
        metadata={"description": "Embeddings for trusted metadata records"}
    )
    
    # Use default text embedding function
    text_embedder = embedding_functions.DefaultEmbeddingFunction()
    
    # -----------------------
    #  Avoid duplicates — check existing UUIDs
    # -----------------------
    existing_count = collection.count()
    
    # Get all existing UUIDs here stored as 'ids' (if collection is not empty)
    existing_ids = []
    if existing_count > 0:
        batch_size = 500
        offset = 0
        while True:
            batch = collection.get(limit=batch_size, offset=offset)
            if not batch["ids"]:
                break
            existing_ids.extend(batch["ids"])
            offset += batch_size
    
    existing_ids = set(existing_ids)
    print(f" Existing embeddings in collection: {len(existing_ids)}")
    
    # Filter new rows
    new_df = metadata_df[~metadata_df["uuid"].isin(existing_ids)]
    print(f" New records to embed: {len(new_df)}")
    
    # -----------------------
    #   Generate and add embeddings
    # -----------------------
    if new_df.empty:
        print(" No new metadata to embed. No data was added.")
    else:
        texts = new_df["combined_text"].tolist()
        uuids = new_df["uuid"].tolist()
    
        print(" Generating embeddings...")
        embeddings = text_embedder(texts)
    
        # setting a barch size to avoid exceeding the maximum allowed limit(5461)
        batch_size = 5000
        total = len(uuids)
        start_idx = 0
    
        print(f" Adding {total} new embeddings to collection '{COLLECTION_NAME}' in batches...")
    
        while start_idx < total:
            end_idx = min(start_idx + batch_size, total)
    
            try:
                collection.add(
                    ids=uuids[start_idx:end_idx],
                    embeddings=embeddings[start_idx:end_idx],
                    metadatas=new_df.iloc[start_idx:end_idx].to_dict(orient="records"),
                    documents=texts[start_idx:end_idx],
                )
                print(f"  Added records {start_idx + 1}-{end_idx} / {total}")
                start_idx = end_idx  # move to next batch
    
            except Exception as e:
                print(f"   Batch {start_idx + 1}-{end_idx} failed: {e}")
    
                # If batch too big, reduce size and retry
                if "max batch size" in str(e).lower() and batch_size > 1000:
                    old_batch = batch_size
                    batch_size = batch_size // 2
                    print(f"  Reducing batch size from {old_batch} -> {batch_size} and retrying...")
                else:
                    raise
    
        print(f"  Added {total} new embeddings to collection '{COLLECTION_NAME}'.")
    
    
    #  Summary
    final_count = collection.count()
    added_count = final_count - existing_count
    
    print("\n ===== Summary =====")
    print(f" Collection: {COLLECTION_NAME}")
    print(f" Previously had: {existing_count}")
    print(f" New added: {added_count}")
    print(f" Total now: {final_count}")
    print("=======================")
    print(" Exploitation Metadata processing complete.")

process_exploitation_metadata();
